{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "se8X_IWBqJgE"
   },
   "source": [
    "## **Keyphrase summarizer which extracts unigram, bigram and trigram keyphrases from product reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-PXZhiuqJgG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import jellyfish   # for Levenshtein distance\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize   # for sentence tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# install nltk packages\n",
    "nltk_downloader = nltk.downloader.Downloader()\n",
    "if not nltk_downloader.is_installed('punkt'):\n",
    "    nltk_downloader.download('punkt')\n",
    "if not nltk_downloader.is_installed('averaged_perceptron_tagger'):\n",
    "    nltk_downloader.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFnD2QlIqJgV"
   },
   "source": [
    "### **Stopwords list and tokenization functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM8W0doOqJgW"
   },
   "outputs": [],
   "source": [
    "# nltk stoplist is not complete\n",
    "nltk_sw = ['d', 'm', 'o', 's', 't', 'y', 'll', 're', 've', 'ma',\n",
    " \"that'll\", 'ain',\n",
    " \"she's\", \"it's\", \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    " 'isn', \"isn't\", 'aren', \"aren't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    " 'don', \"don't\", 'doesn', \"doesn't\", 'didn', \"didn't\",\n",
    " 'hasn', \"hasn't\", 'haven', \"haven't\", 'hadn', \"hadn't\",\n",
    " 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    " 'shan', \"shan't\", 'shouldn', \"shouldn't\", \"should've\",\n",
    " 'won', \"won't\", 'wouldn', \"wouldn't\", 'couldn', \"couldn't\",\n",
    " 'i', 'me', 'my', 'we', 'our', 'ours', 'you', 'your', 'yours',\n",
    " 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs',\n",
    " 'himself', 'herself', 'itself', 'myself', \n",
    " 'yourself', 'yourselves', 'ourselves', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    " 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    " 'had', 'has', 'have', 'having', 'do', 'does', 'did', 'doing',\n",
    " 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    " 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
    " 'about', 'against', 'between', 'into', 'through', \n",
    " 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    " 'over', 'under', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    " 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    " 'other', 'some', 'such', 'no', 'nor', \n",
    " 'only', 'own', 'same', 'so', 'than', 'too', \n",
    " 'again', 'further', 'then', 'once', 'can', 'will', 'just', \n",
    " 'should', 'now']\n",
    "\n",
    "# removed from nltk stoplist: not, very\n",
    "\n",
    "added_sw = [ \"he's\", \"he'd\", \"she'd\", \"he'll\", \"she'll\", \"you'll\", \n",
    "            \"they'd\", \"could've\", \"would've\", 'could', 'would', \"i'm\", 'im',\n",
    "           \"thatll\", \"shes\", \"youre\", \"youve\", \"youll\", \"youd\",\n",
    "            \"isnt\", \"arent\", \"wasnt\", \"werent\",\n",
    "            \"dont\", \"doesnt\", \"didnt\",\n",
    "            \"hasnt\", \"havent\", \"hadnt\",\n",
    "            \"mightnt\", \"mustnt\", \"neednt\", \n",
    "            \"shant\", \"shouldnt\", \"shouldve\",\n",
    "            \"wont\", \"wouldnt\", \"couldnt\", \n",
    "            'a','b','c','e','f','g','h','i','j','k','l','n','p','q','r','u','v','w','x','z','lol']\n",
    "\n",
    "stop_words = added_sw + nltk_sw\n",
    "\n",
    "punc = ''',.;:?!'\"()[]{}<>|\\/@#^&*_~=+\\n\\t'''  #exclude hyphen, $, %\n",
    "fullstop = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCtVHKF5qJgd"
   },
   "outputs": [],
   "source": [
    "# Input a string\n",
    "# Returns a list of tokens with no stopwords, punctuation, numbers\n",
    "\n",
    "def text_preprocess_clean(review):\n",
    "    for p in punc:\n",
    "        review = review.replace(p,' ')\n",
    "    review = review.lower()\n",
    "    review = review.replace('protectors','protector')\n",
    "    review = review.replace('headphones','headphone')\n",
    "    review = review.replace('iphones','iphone')\n",
    "    review = review.replace('phones','phone')\n",
    "    review = review.replace('mounts','mount')\n",
    "    review = review.replace('stands','stand')\n",
    "    review = review.replace('adapters','adapter')\n",
    "    review = review.replace('chargers','charger')\n",
    "    review = review.replace('cables','cable')\n",
    "    review = review.replace('packs','pack')\n",
    "    review = review.replace('batteries','battery')\n",
    "    review = review.replace('cards','card')\n",
    "    review = review.replace('styluses','stylus')\n",
    "    review = review.replace('kits','kit')\n",
    "    review = review.replace('speakers','speaker')\n",
    "    review = review.replace('docks','dock')\n",
    "    review = review.replace('boosters','booster')\n",
    "    review = review.replace('cases','case')\n",
    "    review = re.sub('\\d+', '', review)\n",
    "    review = word_tokenize(review)\n",
    "    review = [w for w in review if w not in stop_words]\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npj3XFE8qJgj"
   },
   "outputs": [],
   "source": [
    "# Input a string\n",
    "# Returns a list of tokens with punctuation and numbers and stopwords\n",
    "# (punctuation allows us to eliminate meaningless bigrams containing punctuation symbols)\n",
    "\n",
    "def text_preprocess(review):\n",
    "    review = review.replace(fullstop,' . ')\n",
    "    review = review.lower()\n",
    "    review = review.replace(\"'m'\",' am')\n",
    "    review = review.replace(\"'s'\",' is')\n",
    "    review = review.replace(\"'re'\",' are')\n",
    "    review = review.replace(\"'ve'\",' have')\n",
    "    review = review.replace(\"'ll'\",' wi11')\n",
    "    review = review.replace(\"'d'\",'')\n",
    "    review = review.replace(\"n't\",' not')\n",
    "    review = review.replace(\"shan't\",'shall not')\n",
    "    review = review.replace(\"won't\",'will not')\n",
    "    review = review.replace('protectors','protector')\n",
    "    review = review.replace('headphones','headphone')\n",
    "    review = review.replace('phones','phone')\n",
    "    review = review.replace('iphones','iphone')\n",
    "    review = review.replace('mounts','mount')\n",
    "    review = review.replace('stands','stand')\n",
    "    review = review.replace('adapters','adapter')\n",
    "    review = review.replace('chargers','charger')\n",
    "    review = review.replace('cables','cable')\n",
    "    review = review.replace('packs','pack')\n",
    "    review = review.replace('batteries','battery')\n",
    "    review = review.replace('cards','card')\n",
    "    review = review.replace('styluses','stylus')\n",
    "    review = review.replace('kits','kit')\n",
    "    review = review.replace('speakers','speaker')\n",
    "    review = review.replace('docks','dock')\n",
    "    review = review.replace('boosters','booster')\n",
    "    review = review.replace('cases','case')\n",
    "    review = word_tokenize(review)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVqjgLiOqJg0"
   },
   "source": [
    "### **Calculate idf for all words in the corpus, excluding stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary word_df (include stopwords)\n",
    "# dataframe = df_allreview\n",
    "\n",
    "def compute_idf(dataframe):\n",
    "    df = dataframe\n",
    "\n",
    "    vocabulary = set()               # corpus vocabulary including stopwords\n",
    "    doc_f = defaultdict(lambda: 0)   # dictionary {word : num of products whose reviews contain the word (document frequency)}\n",
    "    idf = dict()                     # dictionary {word : idf}\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        t1 = text_preprocess_clean(row['all_reviews'])      \n",
    "        vocabulary.update(t1)                     \n",
    "    \n",
    "        t2 = set(text_preprocess_clean(row['all_reviews']))  \n",
    "        for t in t2:\n",
    "            doc_f[t] += 1\n",
    "    \n",
    "    vocabulary = list(vocabulary)\n",
    "\n",
    "    DOC_COUNT = len(df)                 # DOC COUNT = number of products (each product has an allreviews document)\n",
    "\n",
    "    VOCAB_COUNT = len(vocabulary)      # number of unique words\n",
    "    print(f'Number of words in corpus (excluding stopwords): {VOCAB_COUNT}')\n",
    "    print(f'Number of documents (products): {DOC_COUNT}')\n",
    "    \n",
    "    # Calculate the idf of each word in the vocabulary\n",
    "    for w in vocabulary:\n",
    "        idf[w] = math.log10(DOC_COUNT / float(doc_f[w]))    # log to base 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dataframe(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RUN!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "file1 = os.path.join(DATA_DIR, 'asin_numreviews_review.csv')\n",
    "file2 = os.path.join(DATA_DIR, 'asin_numreviews_allreview.csv')\n",
    "\n",
    "df_review = csv_to_dataframe(file1)\n",
    "df_allreview = csv_to_dataframe(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus (excluding stopwords): 95633\n",
      "Number of documents (products): 10429\n"
     ]
    }
   ],
   "source": [
    "compute_idf(df_allreview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_lEvQKDqJg5"
   },
   "source": [
    "### **Search for and print the product's reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q--hFg_5qJg7"
   },
   "outputs": [],
   "source": [
    "# search for the product with index idx and prints the data for the product\n",
    "# dataf = df_review\n",
    "\n",
    "def search(idx, dataf):\n",
    "    pid = dataf.loc[idx]['asin']\n",
    "    n = dataf.loc[idx]['num_reviews']\n",
    "    \n",
    "    print(f'Index: {idx}')\n",
    "    print(f'Product ID: {pid}')\n",
    "    print(f'Number of reviews: {n}\\n')\n",
    "    print('Sample reviews:\\n')\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        rev = dataf.loc[idx][i+1]\n",
    "        print(f'Review {i}:\\n {rev}\\n')\n",
    "        \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gU_MhNY1qJg_"
   },
   "source": [
    "### **Calculate tf and tf-idf for each word in the product's reviews (excluding stopwords)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmpFzzwgqJhB"
   },
   "outputs": [],
   "source": [
    "# Returns a dictionary {word : tf) for all words (excluding stopwords) of the product\n",
    "# tf = word frequency / total number of words (excluding stopwords)\n",
    "# dataf = df_allreview\n",
    "\n",
    "def word_tfidf(idx, idf, dataf):\n",
    "  \n",
    "    allrev = dataf.loc[idx]['all_reviews']    \n",
    "    \n",
    "    u1 = text_preprocess_clean(allrev)\n",
    "    u2 = set(text_preprocess_clean(allrev))\n",
    "    u2 = list(u2)\n",
    "       \n",
    "    tfreq = defaultdict(lambda: 0)   # {word : freq of word in all_reviews}\n",
    "    tf = defaultdict(lambda: 0)\n",
    "    tfidf = defaultdict(lambda: 0)\n",
    "    \n",
    "    for w in u1:\n",
    "        tfreq[w] += 1                         \n",
    "\n",
    "    for w in u2:\n",
    "        tf[w] = 1 + math.log10(float(tfreq[w]))\n",
    "        if w in idf:\n",
    "            tfidf[w] = tf[w] * idf[w]\n",
    "        else:\n",
    "            tfidf[w] = tf[w] \n",
    "            \n",
    "    return tfreq, tf, tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nQzAqs5qJhG"
   },
   "source": [
    "### **Get all candidate phrases (unigrams, bigrams and trigrams) by tokenization and filter out undesirable candidates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oyAJttQqJhI"
   },
   "outputs": [],
   "source": [
    "# returns all unigrams (excluding stopwords) for the product with index idx\n",
    "# dataf = df_allreview\n",
    "\n",
    "def unigram(idx, dataf):\n",
    "    allrev = dataf.loc[idx]['all_reviews']  # type(allrev) = str\n",
    "    \n",
    "    u = set(text_preprocess_clean(allrev))   \n",
    "    u = list(u)\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LMYMDi8qJhM"
   },
   "outputs": [],
   "source": [
    "# returns all bigrams for the product with index idx\n",
    "# remove and reduce bigrams by checking punctuation and stopwords\n",
    "# dataf = df_allreview\n",
    "\n",
    "def bigram(idx, dataf):    \n",
    "    allrev = dataf.loc[idx]['all_reviews']  \n",
    "    u = text_preprocess(allrev)   \n",
    "    b1 = set(nltk.ngrams(u, 2))  \n",
    "    b1 = list(b1)\n",
    "    \n",
    "    b2 = []\n",
    "\n",
    "    for b in b1:\n",
    "        if (b[0] in stop_words) or (b[1] in stop_words) or (b[0] in punc) or (b[1] in punc):  \n",
    "            continue\n",
    "\n",
    "        if (b[0] not in punc) and (b[1] not in punc) and (b[0] not in stop_words) and (b[1] not in stop_words):\n",
    "            b2.append(b)    \n",
    "            \n",
    "    b2 = list(set(b2))\n",
    "    return b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nqh38vpiqJhS"
   },
   "outputs": [],
   "source": [
    "# returns all trigrams for the product with index idx\n",
    "# remove and reduce trigrams by checking punctuation and stopwords\n",
    "# dataf = df_allreview\n",
    "\n",
    "def trigram(idx, dataf):    \n",
    "    allrev = dataf.loc[idx]['all_reviews']  \n",
    "    u = text_preprocess(allrev)   \n",
    "    t1 = set(nltk.ngrams(u, 3))  \n",
    "    t1 = list(t1)\n",
    "    \n",
    "    t2 = []\n",
    "\n",
    "    for t in t1:\n",
    "        if (t[0] in stop_words) or (t[1] in stop_words) or (t[2] in stop_words) or (t[0] in punc) or (t[1] in punc) or (t[2] in punc):\n",
    "            continue\n",
    "\n",
    "        if (t[0] not in punc) and (t[1] not in punc) and (t[2] not in punc) and (t[0] not in stop_words) and (t[1] not in stop_words) and (t[2] not in stop_words):\n",
    "            t2.append(t)    \n",
    "            \n",
    "    t2 = list(set(t2))\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AStM9s6oqJhZ"
   },
   "source": [
    "### **POS-tag each candidates phrase and select those satisfying certain POS tag patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lxa6ts7WqJhZ"
   },
   "outputs": [],
   "source": [
    "def tagging(tokens):\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "\n",
    "# For bigrams, selects and returns the list of final candidates\n",
    "\n",
    "def candidate_pos(tokens, n):\n",
    "    candidates = []\n",
    "\n",
    "    # Popular phone brands - for bigrams, include it as a candidate if the first word is a phone brand \n",
    "    # (because second word likely to be phone model)\n",
    "    brands = ('nokia','motorola','iphone','samsung','xiaomi','huawei',\n",
    "              'siemens','sony','sonyericsson', 'ericsson',\n",
    "              'palm','blackberry','htc','alcatel','benq','at&t','galaxy',\n",
    "              'apple','asus','casio','google','kyocera','nec','sony','android')\n",
    "    \n",
    "    # JJR - adj comparative, JJS - adj superlative, \n",
    "    # RBR - adverb comparative, RBS - adverb superlative\n",
    "    # CD - cardinal number\n",
    "    unigram_tags = ('NN','NNS','NNP','NNPS')\n",
    "    noun_tags = ('NN','NNS','NNP','NNPS')\n",
    "    adjective_tags = ('JJ','JJR','JJS','CD')\n",
    "    adverb_tags = ('RB','RBR','RBS')  # RB for 'not' and 'very'\n",
    "    \n",
    "    #verb_tags = ('VB','VBD','VBP','VBZ')\n",
    "    \n",
    "    if n == 1:                           # for unigrams\n",
    "        tagged_tokens = tagging(tokens)\n",
    "                   \n",
    "        for t in tagged_tokens:         \n",
    "            if t[0] in brands:\n",
    "                candidates.append(t[0]) \n",
    "            \n",
    "            if t[1] in unigram_tags:\n",
    "                candidates.append(t[0])\n",
    "\n",
    "    if n == 2:                          # for bigrams\n",
    "        for x in tokens:\n",
    "            t = tagging(x)\n",
    "            \n",
    "            if x[0] in brands:\n",
    "                candidates.append(x) \n",
    "            \n",
    "            if (t[0][1] in noun_tags) and (t[1][1] in noun_tags):\n",
    "                candidates.append(x)                \n",
    "            if (t[0][1] in adjective_tags) and (t[1][1] in noun_tags):\n",
    "                candidates.append(x)\n",
    "            if (t[0][1] in adverb_tags) and (t[1][1] in adjective_tags):\n",
    "                candidates.append(x)\n",
    "\n",
    "    if n == 3:                          # for trigrams\n",
    "        for x in tokens:\n",
    "            t = tagging(x)\n",
    "            \n",
    "            if x[0] in brands or x[1] in brands:\n",
    "                candidates.append(x) \n",
    "            \n",
    "            if (t[0][1] in noun_tags) and (t[1][1] in noun_tags) and (t[2][1] in noun_tags):\n",
    "                candidates.append(x)                \n",
    "            if (t[0][1] in adjective_tags) and (t[1][1] in noun_tags) and (t[2][1] in noun_tags):\n",
    "                candidates.append(x)                \n",
    "            if (t[0][1] in adverb_tags) and (t[1][1] in adjective_tags) and (t[2][1] in noun_tags):\n",
    "                candidates.append(x)                \n",
    "                    \n",
    "    candidates = list(set(candidates))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwyfIZn6qJhe"
   },
   "source": [
    "### **If using tf-idf for scoring, calculate tf-idf score for each candidate phrase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HM4iTFfAqJhe"
   },
   "outputs": [],
   "source": [
    "# Returns a dictionary {word : tfidf} for all words (unigrams) of the product\n",
    "# create 3-element tuple so that it can be combined with bigram tuple for ranking\n",
    "\n",
    "def unigram_tfidf(tokens, tfidf):\n",
    "    u_tfidf = []\n",
    "    \n",
    "    for u in tokens:\n",
    "        tup = (u, tfidf[u])   # Create tuple   \n",
    "        u_tfidf.append(tup)\n",
    "    \n",
    "    return u_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xnnntd7uqJhk"
   },
   "outputs": [],
   "source": [
    "# Returns a dictionary {bigram : tfidf} for all bigrams) of the product\n",
    "\n",
    "def bigram_tfidf(tokens, tfidf):\n",
    "    b_tfidf = []\n",
    "    \n",
    "    for b in tokens:\n",
    "        b = list(b)\n",
    "        tup = (b[0], b[1], tfidf[b[0]] + tfidf[b[1]]) # Create tuple \n",
    "        b_tfidf.append(tup)    \n",
    "      \n",
    "    return b_tfidf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjHrk8WYqJhn"
   },
   "outputs": [],
   "source": [
    "# Returns a dictionary {bigram : tfidf} for all bigrams) of the product\n",
    "\n",
    "def trigram_tfidf(tokens, tfidf):\n",
    "    t_tfidf = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        t = list(t)\n",
    "        tup = (t[0], t[1], t[2], tfidf[t[0]] + tfidf[t[1]] + tfidf[t[2]]) # Create tuple \n",
    "        t_tfidf.append(tup)    \n",
    "      \n",
    "    return t_tfidf    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbWBJw8HqJhs"
   },
   "source": [
    "### **If using tf for scoring, calculate tf score for each candidate phrase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLwz4GnHqJht"
   },
   "outputs": [],
   "source": [
    "def unigram_tf(tokens, tf):\n",
    "    u_tf = []\n",
    "    \n",
    "    for u in tokens:\n",
    "        tup = (u, '', '', tf[u])   # Create tuple   \n",
    "        u_tf.append(tup)\n",
    "            \n",
    "    return u_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwtg6gDUqJhx"
   },
   "outputs": [],
   "source": [
    "def bigram_tf(tokens, tf):\n",
    "    b_tf = []\n",
    "    \n",
    "    for b in tokens:\n",
    "        b = list(b)\n",
    "        tup = (b[0], b[1], '', tf[b[0]] + tf[b[1]]) # Create tuple \n",
    "        b_tf.append(tup)    \n",
    "      \n",
    "    return b_tf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7ocg7pSqJh1"
   },
   "outputs": [],
   "source": [
    "def trigram_tf(tokens, tf):\n",
    "    t_tf = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        t = list(t)\n",
    "        tup = (t[0], t[1], t[2], tf[t[0]] + tf[t[1]] + tf[t[2]]) # Create tuple \n",
    "        t_tf.append(tup)    \n",
    "      \n",
    "    return t_tf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0iDkoH6qJh5"
   },
   "source": [
    "### **Find the review frequency of each candidate phrase (number of reviews of the product containing the phrase)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OyqRtuIqJh6"
   },
   "outputs": [],
   "source": [
    "# returns a list [s, s, ...] if there are matches of s in data. If no match, returns []\n",
    "\n",
    "def string_match(s, data):\n",
    "    data = str(data)\n",
    "    match = re.findall(re.escape(s), data.lower())\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUfHFKABqJh-"
   },
   "outputs": [],
   "source": [
    "# dataf = df_review\n",
    "\n",
    "def unigram_rf(tokens, idx, dataf,RF_WEIGHT):\n",
    "    data = dataf.loc[idx]    \n",
    "    n = data[1]                    # num_reviews\n",
    "    \n",
    "    u_finalscore = []\n",
    "    u_rf = defaultdict(lambda: 0)  # review freq\n",
    "    \n",
    "    for u in tokens:\n",
    "        s = u[0] \n",
    "        \n",
    "        for i in range(2,n+2):\n",
    "            match = string_match(s, data[i])\n",
    "            if len(match) != 0:\n",
    "                u_rf[u] += 1\n",
    "        \n",
    "        if u not in u_rf:\n",
    "            score = 0\n",
    "        else:                \n",
    "            score = RF_WEIGHT * math.log10(u_rf[u]) + (u_rf[u]/n)   # give recurring unigrams more weight by * 2\n",
    "        \n",
    "        finalscore = score + u[3]        \n",
    "        tup = (u[0], '', '', finalscore) \n",
    "        u_finalscore.append(tup)        \n",
    "                \n",
    "    return u_finalscore                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBcqBLy1qJiB"
   },
   "outputs": [],
   "source": [
    "# dataf = df_review\n",
    "\n",
    "def bigram_rf(tokens, idx, dataf,RF_WEIGHT):\n",
    "    data = dataf.loc[idx]    \n",
    "    n = data[1]                    # num_reviews\n",
    "    \n",
    "    b_finalscore = []\n",
    "    b_rf = defaultdict(lambda: 0)  # review freq\n",
    "    \n",
    "    for b in tokens:\n",
    "        s = b[0] + ' ' + b[1]\n",
    "        \n",
    "        for i in range(2,n+2):\n",
    "            match = string_match(s, data[i])\n",
    "            if len(match) != 0:\n",
    "                b_rf[b] += 1  \n",
    "                \n",
    "        if b not in b_rf:\n",
    "            score = 0\n",
    "        else:                \n",
    "            score = RF_WEIGHT * math.log10(b_rf[b]) + (b_rf[b]/n)\n",
    "\n",
    "        finalscore = score + b[3]\n",
    "        tup = (b[0], b[1], '', finalscore) \n",
    "        b_finalscore.append(tup)\n",
    "        \n",
    "    return b_finalscore  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WcuL5z5VqJiH"
   },
   "outputs": [],
   "source": [
    "# dataf = df_review\n",
    "\n",
    "def trigram_rf(tokens, idx, dataf):\n",
    "    data = dataf.loc[idx]    \n",
    "    n = data[1]                    # num_reviews\n",
    "    \n",
    "    t_finalscore = []\n",
    "    t_rf = defaultdict(lambda: 0)  # review freq\n",
    "    \n",
    "    for t in tokens:\n",
    "        s = t[0] + ' ' + t[1] + ' ' + t[2]\n",
    "        \n",
    "        for i in range(2,n+2):\n",
    "            match = string_match(s, data[i])\n",
    "            if len(match) != 0:\n",
    "                t_rf[t] += 1  \n",
    "                \n",
    "        if t not in t_rf:\n",
    "            score = 0\n",
    "        else:                \n",
    "            score = math.log10(t_rf[t]) + (t_rf[t]/n)\n",
    "\n",
    "        finalscore = score + t[3]\n",
    "        tup = (t[0], t[1], t[2], finalscore) \n",
    "        t_finalscore.append(tup)\n",
    "        \n",
    "    return t_finalscore  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9y0Ws4acqJiM"
   },
   "source": [
    "### **Rank the final candidate phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctLDeftwqJiN"
   },
   "outputs": [],
   "source": [
    "# Returns number of words/phrases to output in summary\n",
    "\n",
    "def num_words(idx, dataf,SUMMARY_SIZE_FACTOR):\n",
    "\n",
    "    # get total number of words in all reviews (excluding stopwords)\n",
    "    allrev = dataf.loc[idx]['all_reviews']  \n",
    "    u = text_preprocess_clean(allrev)   \n",
    "    n = len(u)\n",
    "    numwords =  math.ceil(SUMMARY_SIZE_FACTOR * math.log10(n))  # set number of key phrases in summary\n",
    "    return numwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKPyO4dPqJiS"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of words/phrases in the summary based on the number of unique words in all reviews of the product\n",
    "# Returns the summary words/phrases\n",
    "# dataf = df_allreview\n",
    "\n",
    "def rank_score(idx, utokens, btokens, ttokens, dataf,SUMMARY_SIZE_FACTOR):    \n",
    "    candidates = []\n",
    "    result = []\n",
    "    \n",
    "    summary_size = num_words(idx, dataf,SUMMARY_SIZE_FACTOR)\n",
    "    \n",
    "    # Concatenate the lists of unigrams and bigrams\n",
    "    candidates = utokens + btokens + ttokens\n",
    "    candidates = list(set(candidates))\n",
    "    \n",
    "    candidates_sorted = sorted(candidates, key=lambda tup: tup[3], reverse=True)    \n",
    "    n = min(len(candidates_sorted), summary_size)\n",
    "              \n",
    "    for i in range(0,n):\n",
    "        result.append(candidates_sorted[i])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8azjbcOfqJiV"
   },
   "source": [
    "### **Calculate and check Levenshtein distance between each pair of candidate phrases**\n",
    "### **Filter out very similar phrases and return the final summary phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXTV_mZdqJiV"
   },
   "outputs": [],
   "source": [
    "# Levenshtein distance for each pair of candidate words (unigrams, bigrams, trigrams)\n",
    "# Instead of stemming the words at the beginning, we use this method to remove words that are too similar \n",
    "\n",
    "def levenshtein(w1, w2):\n",
    "    lev_dist = 1. - jellyfish.levenshtein_distance(w1, w2) / max(len(w1), len(w2))\n",
    "\n",
    "    return lev_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6jB7tIgqJiZ"
   },
   "outputs": [],
   "source": [
    "# Checks Levenshtein distance between each pair of words/phrases\n",
    "# If distance >= 0.8, reject the SHORTER word/phrase. If both words are the same length, do not reject any.\n",
    "# Return a list of candidates as final results\n",
    "\n",
    "def check_similarity(words, LEVENSHTEIN_THRESHOLD):\n",
    "    tokens = []\n",
    "    reject = []    \n",
    "    result = {}\n",
    "    length = len(words)\n",
    "\n",
    "    # Convert tuples to strings\n",
    "    for tup in words:\n",
    "        s = tup[0] + ' ' + tup[1] + ' ' + tup[2]\n",
    "        tokens.append(s.strip())\n",
    "        \n",
    "    # Check similarity between every pair of terms\n",
    "    # If similarity >= 0.8, reject it as a candidate\n",
    "    for i, w in enumerate(tokens):\n",
    "        for j in range(0,length):\n",
    "            if i != j and levenshtein(w, tokens[j]) >= LEVENSHTEIN_THRESHOLD:\n",
    "                if len(w) < len(tokens[j]):\n",
    "                    reject.append(w)\n",
    "                    \n",
    "                if len(w) > len(tokens[j]):\n",
    "                    reject.append(tokens[j])\n",
    "\n",
    "    # Remove rejected strings     \n",
    "    result = list(set(tokens) - set(reject))\n",
    "    \n",
    "    return sorted(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97-QvgGwqJi7"
   },
   "source": [
    "### **Main function for keyphrase summarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GCRqqrmqJi-"
   },
   "outputs": [],
   "source": [
    "def main(idx):\n",
    "    \n",
    "    if idx >= 10429:\n",
    "        print('Please enter a product id from 0 to 10428.')\n",
    "        return\n",
    "    \n",
    "    # ADJUSTABLE PARAMETERS\n",
    "    SUMMARY_SIZE_FACTOR = 5      # numwords =  math.ceil(SUMMARY_SIZE_FACTOR * math.log10(n)\n",
    "    RF_WEIGHT = 2                # weight for unigrams and bigrams when calculating review frequency score\n",
    "    LEVENSHTEIN_THRESHOLD = 0.8  # reject one candidate if threshold distance between two candidates >= 0.8  \n",
    "    \n",
    "    print('**** PRODUCT REVIEW SUMMARIZER ****\\n')\n",
    "\n",
    "    # STEP 1: search for the index and print data (index, product id, number of reviews, review text)\n",
    "    num_reviews = search(idx, df_review)\n",
    "    \n",
    "    # STEP 2: calculate tf-idf for product reviews' words\n",
    "    tfreq, tf, tfidf = word_tfidf(idx, idf, df_allreview)\n",
    "    \n",
    "    # STEP 3: tokenize and filter tokens to select candidates\n",
    "    u1 = unigram(idx, df_allreview)\n",
    "    b1 = bigram(idx, df_allreview)\n",
    "    t1 = trigram(idx, df_allreview)\n",
    " \n",
    "    # STEP 4: filter candidates by POS tags    \n",
    "    u2 = candidate_pos(u1, 1)\n",
    "    b2 = candidate_pos(b1, 2)\n",
    "    t2 = candidate_pos(t1, 3)\n",
    "    \n",
    "    # STEP 5.1: Use TF-IDF to calculate the score for each candidate\n",
    "    u3 = unigram_tfidf(u2, tfidf)\n",
    "    b3 = bigram_tfidf(b2, tfidf)  \n",
    "    t3 = trigram_tfidf(t2, tfidf)  \n",
    "        \n",
    "    # STEP 5.2: Use TF to calculate the score for each candidate\n",
    "    u4 = unigram_tf(u2, tf)\n",
    "    b4 = bigram_tf(b2, tf)  \n",
    "    t4 = trigram_tf(t2, tf)  \n",
    "    \n",
    "    # STEP 6: Count the number of reviews of the product each candidate is in. \n",
    "    u5 = unigram_rf(u4, idx, df_review, RF_WEIGHT)\n",
    "    b5 = bigram_rf(b4, idx, df_review, RF_WEIGHT)\n",
    "    t5 = trigram_rf(t4, idx, df_review)\n",
    "        \n",
    "    # STEP 7: Select top-ranked candidates by final score and rank the candidates\n",
    "    result = rank_score(idx, u5, b5, t5, df_allreview, SUMMARY_SIZE_FACTOR)      \n",
    "                      \n",
    "    # STEP 8: Check Levenshtein distance to filter out very similar words (such as singular and plural forms). \n",
    "    summary = check_similarity(result, LEVENSHTEIN_THRESHOLD)\n",
    "    \n",
    "    print('\\nSUMMARY KEYPHRASES:\\n')\n",
    "    for s in summary:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tagywKBNqJjC",
    "outputId": "c61f5209-d12e-4e5d-8ed6-893f38a9b1db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** PRODUCT REVIEW SUMMARIZER ****\n",
      "\n",
      "Index: 1001\n",
      "Product ID: B0087GALZI\n",
      "Number of reviews: 38\n",
      "\n",
      "Sample reviews:\n",
      "\n",
      "Review 1:\n",
      " Everyone asks about it, it's just right for my GS3 and protects it well. Would buy again , love the style and color\n",
      "\n",
      "Review 2:\n",
      " Fits perfect on the cell phone and I love the design and colors.  Doesn't slip so stays put.  Fast and packaged good.\n",
      "\n",
      "Review 3:\n",
      " I am impressed with this cover/case, I purchased two, one purple and one clear. They are both consistent in the way they fit my phone. I also purchased the Incipio SA-301 Feather Ultra-Light Hard Shell Case, which I love, but I feel a bit more secure with the Cimo in regards to dropping the phone and it landing face down. The Cimo has more of a 'rim' of protection around the front and it seems it may provide enough height that it could save the face of the phone. The cover/case has a nice feel, not a gummy or dull appearance, and it is flexible while still giving 'hard' case protection.\n",
      "\n",
      "\n",
      "SUMMARY KEYPHRASES:\n",
      "\n",
      "case\n",
      "case fit\n",
      "cruzerlite tpu case\n",
      "different tpu case\n",
      "matte tpu case\n",
      "much better case\n",
      "nice simple case\n",
      "phone\n",
      "silicon feeling case\n",
      "slim fit case\n",
      "smoke color case\n",
      "softest tpu case\n",
      "tpu case\n",
      "tpu case makers\n",
      "transparent cover case\n",
      "very good case\n",
      "white phone\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Enter the index of product (0 to 10428) to retrieve summary keyphrases.\n",
    "'''\n",
    "product_idx = 1001\n",
    "main(product_idx)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AI6122_G01_PART2_SUMMARIZER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
